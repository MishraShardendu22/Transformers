{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MishraShardendu22/Transformers/blob/main/Translate_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch -y\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "-s3WHocWFOwS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "stIWG841FOak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed736cc-fd29-49e9-9dd3-73b0ddbf222b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IXxQHCoByXog"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece accelerate evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaination (ai help)\n",
        "\n",
        "```python\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "```\n",
        "\n",
        "### What it does\n",
        "\n",
        "It splits your dataset into two parts:\n",
        "\n",
        "* **90% → training set**\n",
        "* **10% → validation (test) set**\n",
        "\n",
        "Since you selected **30,000 samples**:\n",
        "\n",
        "* 27,000 → used to train the model\n",
        "* 3,000 → used to evaluate model performance\n",
        "\n",
        "---\n",
        "\n",
        "### Why this is required\n",
        "\n",
        "During training:\n",
        "\n",
        "* Model learns on the **train set**\n",
        "* After each epoch, performance is checked on the **validation set**\n",
        "* Prevents overfitting\n",
        "* Lets you measure BLEU score properly\n",
        "\n",
        "---\n",
        "\n",
        "### What `print(dataset)` shows\n",
        "\n",
        "You will see something like:\n",
        "\n",
        "```\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ...\n",
        "        num_rows: 27000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ...\n",
        "        num_rows: 3000\n",
        "    })\n",
        "})\n",
        "```"
      ],
      "metadata": {
        "id": "p5Hb88nvNnkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Correct dataset name\n",
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "\n",
        "# Shuffle and take 100,000 samples\n",
        "dataset = dataset[\"train\"].shuffle(seed=42).select(range(100_000))\n",
        "\n",
        "# Train-validation split\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "LBH8rvVi50R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    EncoderDecoderConfig,\n",
        "    EncoderDecoderModel,\n",
        "    BertConfig\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "encoder_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        "    max_position_embeddings=512,\n",
        ")\n",
        "\n",
        "decoder_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=512,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=8,\n",
        "    intermediate_size=2048,\n",
        "    is_decoder=True,\n",
        "    add_cross_attention=True,\n",
        "    max_position_embeddings=512,\n",
        ")\n",
        "\n",
        "config = EncoderDecoderConfig.from_encoder_decoder_configs(\n",
        "    encoder_config,\n",
        "    decoder_config\n",
        ")\n",
        "\n",
        "model = EncoderDecoderModel(config=config)\n",
        "\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "xyBGu7-T6vdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [x[\"hi\"] for x in examples[\"translation\"]]\n",
        "    targets = [x[\"en\"] for x in examples[\"translation\"]]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    num_proc=6\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "6Rzn7NKtAJP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model\n",
        ")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./scratch-hi-en\",\n",
        "    eval_strategy=\"steps\", # Changed from evaluation_strategy\n",
        "    save_strategy=\"steps\",\n",
        "    logging_steps=1000,\n",
        "    save_steps=4000,\n",
        "    eval_steps=4000,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready\")"
      ],
      "metadata": {
        "id": "hbsZPWHbAJrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac953bc-2c3e-4676-8a1b-4b786334c8de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "rGYiy9AyAn4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the use of these lines ?\n",
        "\n",
        "Those lines define **special tokens required for generation** in an encoder–decoder model.\n",
        "\n",
        "Your scratch model does not automatically know:\n",
        "\n",
        "* where decoding starts\n",
        "* where it ends\n",
        "* what padding token is\n",
        "\n",
        "So you manually set them.\n",
        "\n",
        "---\n",
        "\n",
        "## 1️⃣ `decoder_start_token_id`\n",
        "\n",
        "```python\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "```\n",
        "\n",
        "Tells the decoder:\n",
        "\n",
        "> Start generation with this token.\n",
        "\n",
        "Without it → generation fails.\n",
        "\n",
        "It is the first token fed into decoder at time step 0.\n",
        "\n",
        "---\n",
        "\n",
        "## 2️⃣ `bos_token_id`\n",
        "\n",
        "```python\n",
        "model.config.bos_token_id = tokenizer.cls_token_id\n",
        "```\n",
        "\n",
        "BOS = Beginning Of Sentence.\n",
        "\n",
        "Used internally by generation utilities.\n",
        "\n",
        "For BERT tokenizer:\n",
        "\n",
        "* `cls_token_id` works as BOS.\n",
        "\n",
        "---\n",
        "\n",
        "## 3️⃣ `eos_token_id`\n",
        "\n",
        "```python\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "```\n",
        "\n",
        "EOS = End Of Sentence.\n",
        "\n",
        "When model predicts this token:\n",
        "→ generation stops.\n",
        "\n",
        "Without EOS → model may generate endlessly.\n",
        "\n",
        "---\n",
        "\n",
        "## 4️⃣ `pad_token_id`\n",
        "\n",
        "```python\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "```\n",
        "\n",
        "Used for:\n",
        "\n",
        "* Padding batches\n",
        "* Ignoring padded tokens in loss\n",
        "* Beam search masking\n",
        "\n",
        "---\n",
        "\n",
        "## Why this was necessary\n",
        "\n",
        "Because you built model **from scratch config**, not from pretrained checkpoint.\n",
        "\n",
        "Pretrained models already contain these IDs.\n",
        "Scratch config does not.\n",
        "\n",
        "Without setting these:\n",
        "\n",
        "* `generate()` throws errors\n",
        "* Decoding behaves incorrectly\n",
        "\n",
        "---\n",
        "\n",
        "## 1️⃣ `tokenizer.cls_token_id`\n",
        "\n",
        "This is the ID of the `[CLS]` token.\n",
        "\n",
        "Example (mBERT):\n",
        "\n",
        "```python\n",
        "tokenizer.cls_token        → \"[CLS]\"\n",
        "tokenizer.cls_token_id     → 101\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* `[CLS]` is stored in vocabulary\n",
        "* It has fixed integer ID\n",
        "* That integer is used inside tensors\n",
        "\n",
        "You used it as:\n",
        "\n",
        "* decoder start token\n",
        "* beginning-of-sentence token\n",
        "\n",
        "---\n",
        "\n",
        "## 2️⃣ `tokenizer.sep_token_id`\n",
        "\n",
        "This is the ID of `[SEP]`.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "tokenizer.sep_token        → \"[SEP]\"\n",
        "tokenizer.sep_token_id     → 102\n",
        "```\n",
        "\n",
        "Used as:\n",
        "\n",
        "* end-of-sentence marker\n",
        "\n",
        "When model predicts ID 102 → generation stops.\n",
        "\n",
        "---\n",
        "\n",
        "## 3️⃣ `tokenizer.pad_token_id`\n",
        "\n",
        "This is the ID of `[PAD]`.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "tokenizer.pad_token        → \"[PAD]\"\n",
        "tokenizer.pad_token_id     → 0\n",
        "```\n",
        "\n",
        "Used to:\n",
        "\n",
        "* fill shorter sequences\n",
        "* ignore padded positions during loss\n",
        "* mask attention\n",
        "\n",
        "---\n",
        "\n",
        "## Why use tokenizer IDs?\n",
        "\n",
        "Because:\n",
        "\n",
        "* Model works with integers, not strings.\n",
        "* Vocabulary mapping is defined inside tokenizer.\n",
        "* These IDs must match tokenizer vocabulary exactly.\n",
        "\n",
        "If mismatched → decoding breaks.\n"
      ],
      "metadata": {
        "id": "UHhvd0mppzbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "model.config.bos_token_id = tokenizer.cls_token_id"
      ],
      "metadata": {
        "id": "wpAPiBOMf0c1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"तुम कौन हो? तुम यहाँ क्या कर रहे हो? तुम्हारे यहाँ होने का क्या कारण है?\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=64,\n",
        "    truncation=True\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=64,\n",
        "    decoder_start_token_id=tokenizer.cls_token_id,\n",
        "    bos_token_id=tokenizer.cls_token_id,\n",
        "    eos_token_id=tokenizer.sep_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Translation:\", translation)"
      ],
      "metadata": {
        "id": "c17qp8X1AiFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99bfb73-2b15-491b-8031-3ab24294dea3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: \" You will be said : \" I have to you? \"\n"
          ]
        }
      ]
    }
  ]
}